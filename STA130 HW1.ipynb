{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94291452",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590eeffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the URL of a sample CSV file\n",
    "csv_url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv(csv_url)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isna().sum()\n",
    "\n",
    "# Display missing values per column\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313bc8c9",
   "metadata": {},
   "source": [
    "# Q2(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "659f2e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 891 rows and 15 columns.\n"
     ]
    }
   ],
   "source": [
    "# Get the dimensions of the dataset\n",
    "dimensions = df.shape\n",
    "print(f\"The dataset has {dimensions[0]} rows and {dimensions[1]} columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f41279",
   "metadata": {},
   "source": [
    "# Q2(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df2b03",
   "metadata": {},
   "source": [
    "Observations: These are the individual entries in a dataset, shown as rows. For example, in a Titanic dataset, each row represents one passenger and their details.\n",
    "\n",
    "Variables: These are the types of information recorded for each entry, shown as columns. In the Titanic dataset, variables could include a passengerâ€™s age, name, gender, or the price they paid for their ticket.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea95fc",
   "metadata": {},
   "source": [
    "# Q3 The suggested code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad7d547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Summary:\n",
      "          survived      pclass         age       sibsp       parch        fare\n",
      "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n",
      "\n",
      "Value Counts for 'sex':\n",
      " sex\n",
      "male      577\n",
      "female    314\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value Counts for 'class':\n",
      " class\n",
      "Third     491\n",
      "First     216\n",
      "Second    184\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Summary of numeric columns\n",
    "numeric_summary = df.describe()\n",
    "\n",
    "# Summary of value counts for a specific column (e.g., 'sex')\n",
    "value_counts_sex = df['sex'].value_counts()\n",
    "\n",
    "# Summary of value counts for another categorical column (e.g., 'class')\n",
    "value_counts_class = df['class'].value_counts()\n",
    "\n",
    "# Display summaries\n",
    "print(\"Numeric Summary:\\n\", numeric_summary)\n",
    "print(\"\\nValue Counts for 'sex':\\n\", value_counts_sex)\n",
    "print(\"\\nValue Counts for 'class':\\n\", value_counts_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce86a8b6",
   "metadata": {},
   "source": [
    "# The summary of my dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad4655",
   "metadata": {},
   "source": [
    "Numeric Summary:\n",
    "          survived      pclass         age       sibsp       parch        fare\n",
    "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
    "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
    "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
    "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
    "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
    "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
    "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
    "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n",
    "\n",
    "Value Counts for 'sex':\n",
    " sex\n",
    "male      577\n",
    "female    314\n",
    "Name: count, dtype: int64\n",
    "\n",
    "Value Counts for 'class':\n",
    " class\n",
    "Third     491\n",
    "First     216\n",
    "Second    184\n",
    "Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d56eef78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(survived         int64\n",
       " pclass           int64\n",
       " sex             object\n",
       " age            float64\n",
       " sibsp            int64\n",
       " parch            int64\n",
       " fare           float64\n",
       " embarked        object\n",
       " class           object\n",
       " who             object\n",
       " adult_male        bool\n",
       " deck            object\n",
       " embark_town     object\n",
       " alive           object\n",
       " alone             bool\n",
       " dtype: object,\n",
       " survived      0\n",
       " pclass        0\n",
       " age         177\n",
       " sibsp         0\n",
       " parch         0\n",
       " fare          0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from the provided URL\n",
    "df = pd.read_csv(csv_url)\n",
    "\n",
    "# Checking the data types of each column to identify non-numeric variables\n",
    "data_types = df.dtypes\n",
    "\n",
    "# Filtering numeric columns and checking for missing values\n",
    "numeric_columns = df.select_dtypes(include=['number'])\n",
    "missing_in_numeric = numeric_columns.isna().sum()\n",
    "\n",
    "data_types, missing_in_numeric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9cdb15",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56b1826c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 15),\n",
       " survived         int64\n",
       " pclass           int64\n",
       " sex             object\n",
       " age            float64\n",
       " sibsp            int64\n",
       " parch            int64\n",
       " fare           float64\n",
       " embarked        object\n",
       " class           object\n",
       " who             object\n",
       " adult_male        bool\n",
       " deck            object\n",
       " embark_town     object\n",
       " alive           object\n",
       " alone             bool\n",
       " dtype: object,\n",
       " survived         0\n",
       " pclass           0\n",
       " sex              0\n",
       " age            177\n",
       " sibsp            0\n",
       " parch            0\n",
       " fare             0\n",
       " embarked         2\n",
       " class            0\n",
       " who              0\n",
       " adult_male       0\n",
       " deck           688\n",
       " embark_town      2\n",
       " alive            0\n",
       " alone            0\n",
       " dtype: int64,\n",
       "          survived      pclass         age       sibsp       parch        fare\n",
       " count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
       " mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
       " std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
       " min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
       " 25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
       " 50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
       " 75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
       " max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the URL of a sample CSV file\n",
    "csv_url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv(csv_url)\n",
    "\n",
    "# Check the shape of the dataset\n",
    "shape = df.shape\n",
    "\n",
    "# Check column data types\n",
    "column_dtypes = df.dtypes\n",
    "\n",
    "# Check the missing values in each column\n",
    "missing_values = df.isna().sum()\n",
    "\n",
    "# Check the summary statistics (only numeric columns)\n",
    "numeric_summary = df.describe()\n",
    "\n",
    "# Check the summary statistics for all columns (including non-numeric)\n",
    "all_summary = df.describe(include='all')\n",
    "\n",
    "shape, column_dtypes, missing_values, numeric_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8fc587",
   "metadata": {},
   "source": [
    "The dataset, as indicated by df.shape, contains 15 columns (891 rows and 15 columns in total). However, when using df.describe(), only 6 columns (survived, pclass, age, sibsp, parch, and fare) are analyzed, all of which are numeric. Non-numeric columns, such as sex, embarked, deck, and who, are excluded since df.describe() by default calculates statistics only for numeric variables. This leads to a difference between the 15 columns reported by df.shape and the 6 columns analyzed by df.describe().\n",
    "\n",
    "In terms of the \"count\" column in df.describe(), it reflects the number of non-missing values in each numeric variable. For example, the age column has 714 non-missing values, as shown by the \"count,\" while the dataset contains 891 rows in total. The discrepancy arises because there are 177 missing values in the age column (891 - 714 = 177 missing values, which is confirmed by df.isna().sum()). Other columns such as survived, pclass, sibsp, parch, and fare do not have any missing values, so their \"count\" remains at 891, matching the number of rows in the dataset.\n",
    "\n",
    "In summary, df.describe() only analyzes numeric columns, while df.shape accounts for both numeric and non-numeric columns. Additionally, the \"count\" in df.describe() shows the number of non-missing values for each numeric column, which may be less than the total number of rows when missing data is present, as is the case with the age column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d7f04",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa2fc5",
   "metadata": {},
   "source": [
    "\"df.describe()\" is a method that calculates and shows summary statistics for the DataFrame, such as count, mean, std (standard deviation), min, 25th percentile (Q1), 50th percentile (median), 75th percentile (Q3), and max. It does some work every time you run it. \n",
    "\n",
    "\"df.shape\" is a attribute gives you the size of the DataFrame (rows and columns) without doing any work. It just shows information thatâ€™s already there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd025b",
   "metadata": {},
   "source": [
    "# summaries of ChatBot session of all prelecture question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0417089f",
   "metadata": {},
   "source": [
    "Here is a summary of our exchanges, which you can use for your homework assignment:\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Interactions:\n",
    "\n",
    "1. **Dataset Loading and Initial Inspection**:\n",
    "   - You provided the URL to the Titanic dataset hosted on GitHub. After loading the dataset into a Pandas DataFrame, we explored various properties.\n",
    "   - I explained how to check for missing values and non-numeric columns using `df.isna().sum()` and `df.dtypes`, respectively.\n",
    "   - You ran the code yourself and found the dataset contained both numeric and non-numeric variables, including some missing values in the `age` column.\n",
    "\n",
    "2. **Dataset Properties**:\n",
    "   - We discussed two important concepts: **observations** (rows in the dataset) and **variables** (columns in the dataset).\n",
    "   - You learned how to determine the shape of the dataset with `df.shape`, which gives the number of rows and columns, and how to summarize it with `df.describe()`, which provides statistical information about the numeric columns.\n",
    "   - I also demonstrated how to count unique values in categorical columns using `df['column'].value_counts()`.\n",
    "\n",
    "3. **Handling Discrepancies between `df.shape` and `df.describe()`**:\n",
    "   - We discussed why `df.shape` and `df.describe()` might report different numbers of columns. `df.shape` counts all columns, whereas `df.describe()` focuses on numeric columns unless explicitly told to include all types.\n",
    "   - We also examined why the \"count\" in `df.describe()` can be less than the number of rows, particularly when a column contains missing values (like `age` with 177 missing entries in the Titanic dataset).\n",
    "\n",
    "4. **Attributes vs Methods**:\n",
    "   - I explained the difference between an **attribute** and a **method** in Python:\n",
    "     - An **attribute** (e.g., `df.shape`) is a property that stores information about the object (like its dimensions) and is accessed without parentheses.\n",
    "     - A **method** (e.g., `df.describe()`) is a function associated with the object that performs an action or computation and is called with parentheses.\n",
    "   - This distinction helped clarify why you use `()` with some commands and not others.\n",
    "\n",
    "---\n",
    "\n",
    "This summary captures the key points of our discussions related to your dataset analysis and Python concepts. Let me know if you'd like to add or modify any parts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca38bba",
   "metadata": {},
   "source": [
    "# link to chat log histories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cbd395",
   "metadata": {},
   "source": [
    "https://chatgpt.com/c/66e25c06-71ac-8001-95b8-932e2523b82e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc005e7c",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c591b2",
   "metadata": {},
   "source": [
    "count: Indicates the number of non-missing (non-NA) values for each variable, showing how many data points are available for each column.\n",
    "\n",
    "mean: The average value for each column, calculated by dividing the sum of all values by the total count.\n",
    "\n",
    "std (Standard Deviation): Reflects the variability or spread of the data, showing how much the values deviate from the mean. A larger standard deviation suggests more variation in the data.\n",
    "\n",
    "min: The lowest value in each column.\n",
    "\n",
    "25% (First Quartile): The value below which 25% of the data lies, also known as the first quartile (Q1).\n",
    "\n",
    "50% (Median): The midpoint of the dataset, with half of the values being lower and half being higher. This is also called the second quartile (Q2).\n",
    "\n",
    "75% (Third Quartile): The value below which 75% of the data falls, referred to as the third quartile (Q3).\n",
    "\n",
    "max: The highest value in each column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3308e07",
   "metadata": {},
   "source": [
    "# Q7 (1) an example of a \"use case\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a6a359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 15), (182, 15))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows where any column contains NaN values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Check the number of rows and columns before and after dropping NaN rows\n",
    "original_shape = df.shape\n",
    "cleaned_shape = df_cleaned.shape\n",
    "\n",
    "original_shape, cleaned_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ef76c",
   "metadata": {},
   "source": [
    "# Q7 (2) an example of \"the opposite use case\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2595634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
       "       'embarked', 'class', 'who', 'adult_male', 'embark_town', 'alive',\n",
       "       'alone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the 'deck' column\n",
    "del df['deck']\n",
    "\n",
    "# Verify that the column has been removed\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27d0b2",
   "metadata": {},
   "source": [
    "# Q7(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df52edee",
   "metadata": {},
   "source": [
    "Using del df['col'] before calling df.dropna() is important for several reasons:\n",
    "    \n",
    "\n",
    "Avoid Excessive Row Removal: \n",
    "If a column with numerous missing values (like 'deck' in the Titanic dataset) isnâ€™t crucial for your analysis, removing it first prevents unnecessarily dropping many rows. This way, rows with missing values only in irrelevant columns are removed, preserving important data.\n",
    "    \n",
    "\n",
    "Preserve Dataset Quality: \n",
    "Columns with too many missing values can cause significant data loss if kept during df.dropna(). Removing such columns ensures that only rows with missing values in key columns are affected, keeping your dataset more reliable for analysis.\n",
    "\n",
    "Focus Imputation on Relevant Data: \n",
    "Dropping unimportant columns before imputing missing values helps you focus on filling missing values in the columns that matter, streamlining the process.\n",
    "\n",
    "Boost Computational Efficiency: \n",
    "By eliminating unneeded columns, you reduce the dataset size, which can speed up future tasks like data cleaning or model building.\n",
    "\n",
    "In conclusion, deleting columns with many missing values before using df.dropna() helps retain valuable data, avoid unnecessary row drops, and ensure a cleaner dataset for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd21006",
   "metadata": {},
   "source": [
    "# Q7ï¼ˆ4ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b39dd823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "Shape: (891, 14)\n",
      "Missing values:\n",
      " survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "After cleaning:\n",
      "Shape: (712, 14)\n",
      "Missing values:\n",
      " survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the shape and missing data before cleaning\n",
    "before_shape = df.shape\n",
    "before_missing = df.isna().sum()\n",
    "\n",
    "# Drop rows with missing values in 'age', 'embarked', and 'embark_town'\n",
    "df_cleaned = df.dropna(subset=['age', 'embarked', 'embark_town'])\n",
    "\n",
    "# Check the shape and missing data after cleaning\n",
    "after_shape = df_cleaned.shape\n",
    "after_missing = df_cleaned.isna().sum()\n",
    "\n",
    "# Display the before and after information\n",
    "print(\"Before cleaning:\")\n",
    "print(\"Shape:\", before_shape)\n",
    "print(\"Missing values:\\n\", before_missing)\n",
    "\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(\"Shape:\", after_shape)\n",
    "print(\"Missing values:\\n\", after_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db7088",
   "metadata": {},
   "source": [
    "# Before and After Report:\n",
    "Before Cleaning:\n",
    "\n",
    "Shape: (6 rows, 4 columns)\n",
    "Missing values:\n",
    "age: 2 missing values\n",
    "embarked: 1 missing value\n",
    "embark_town: 1 missing value\n",
    "fare: 0 missing values\n",
    "\n",
    "After Cleaning:\n",
    "\n",
    "Shape: (3 rows, 4 columns)\n",
    "Missing values: All columns now have 0 missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c14eb",
   "metadata": {},
   "source": [
    "# Justification for the Approach:\n",
    "I used the df.dropna(subset=['age', 'embarked', 'embark_town']) function to remove rows with missing data in key columns (age, embarked, and embark_town). This approach ensures that the dataset is free of incomplete information in these critical fields, which are likely important for analysis (e.g., age for demographic analysis, embarked for location-based insights). The remaining dataset now contains only rows with full information in these columns, which will help improve the reliability of any analysis performed on this data.\n",
    "\n",
    "By removing incomplete rows, the dataset size reduced from 6 to 3 rows, but the cleaned data is more consistent and usable for further analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089f4d3",
   "metadata": {},
   "source": [
    "# Q8(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e0ffc4",
   "metadata": {},
   "source": [
    "# df.groupby(\"col1\")[\"col2\"].describe():\n",
    "This code serves two primary purposes. First, it organizes the DataFrame (df) by categorizing the data based on the unique values found in the specified \"col1\" column, effectively grouping the dataset into subsets. Second, for each of these groups, the code generates detailed summary statistics for the values in the \"col2\" column. These statistics include key metrics such as count, mean, standard deviation, minimum, and maximum values, as well as the 25th, 50th (median), and 75th percentiles, providing a comprehensive overview of the distribution of col2 within each group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7347f85",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1da354f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>First</th>\n",
       "      <td>216.0</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.484026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Second</th>\n",
       "      <td>184.0</td>\n",
       "      <td>0.472826</td>\n",
       "      <td>0.500623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third</th>\n",
       "      <td>491.0</td>\n",
       "      <td>0.242363</td>\n",
       "      <td>0.428949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count      mean       std  min  25%  50%  75%  max\n",
       "class                                                     \n",
       "First   216.0  0.629630  0.484026  0.0  0.0  1.0  1.0  1.0\n",
       "Second  184.0  0.472826  0.500623  0.0  0.0  0.0  1.0  1.0\n",
       "Third   491.0  0.242363  0.428949  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"class\")[\"survived\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581e939",
   "metadata": {},
   "source": [
    "The Titanic data is grouped by the \"class\" column, and summary statistics are provided for the \"survived\" column. This would show you how many people survived in each class, their mean survival rate, and other statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31dc33",
   "metadata": {},
   "source": [
    "# Q8(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa8290",
   "metadata": {},
   "source": [
    "When using df.describe() without removing missing values, the count shows how many non-empty (non-NaN) entries are in each column. This count changes for each column depending on how much data is missing. The focus is on the entire column, giving a summary of the available data.\n",
    "\n",
    "On the other hand, with df.groupby(\"col1\")[\"col2\"].describe(), the count tells how many non-missing values are in col2 for each group made by the different values in col1. This is different because it shows the number of valid data points for each group, not for the whole column, focusing on the relationship between col1 and col2 within each group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697244c2",
   "metadata": {},
   "source": [
    "# Q8 (3)A wrong  code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154c43aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m csv_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Import the dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(csv_url)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Check for missing values\u001b[39;00m\n\u001b[1;32m      8\u001b[0m missing_values \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the URL of a sample CSV file\n",
    "csv_url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv(csv_url)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isna().sum()\n",
    "\n",
    "# Display missing values per column\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59acc0",
   "metadata": {},
   "source": [
    "Output:\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[1], line 5\n",
    "      2 csv_url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "      4 # Import the dataset\n",
    "----> 5 df = pd.read_csv(csv_url)\n",
    "      7 # Check for missing values\n",
    "      8 missing_values = df.isna().sum()\n",
    "\n",
    "NameError: name 'pd' is not defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d6d8a",
   "metadata": {},
   "source": [
    "# The Right code of A given by Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb09dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Define the URL of a sample CSV file\n",
    "csv_url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "\n",
    "# Import the dataset\n",
    "df = pd.read_csv(csv_url)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isna().sum()\n",
    "\n",
    "# Display missing values per column\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83cfa0",
   "metadata": {},
   "source": [
    "# Q8 (3)B wrong code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7e2621a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Attempting to load a non-existent file due to a typo in the file name\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Attempting to load a non-existent file due to a typo in the file name\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e2735",
   "metadata": {},
   "source": [
    "Output:\n",
    "---------------------------------------------------------------------------\n",
    "HTTPError                                 Traceback (most recent call last)\n",
    "Cell In[5], line 4\n",
    "      1 import pandas as pd\n",
    "      3 # Attempting to load a non-existent file due to a typo in the file name\n",
    "----> 4 df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv')\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n",
    "    935 kwds_defaults = _refine_defaults_read(\n",
    "    936     dialect,\n",
    "    937     delimiter,\n",
    "   (...)\n",
    "    944     dtype_backend=dtype_backend,\n",
    "    945 )\n",
    "    946 kwds.update(kwds_defaults)\n",
    "--> 948 return _read(filepath_or_buffer, kwds)\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611, in _read(filepath_or_buffer, kwds)\n",
    "    608 _validate_names(kwds.get(\"names\", None))\n",
    "    610 # Create the parser.\n",
    "--> 611 parser = TextFileReader(filepath_or_buffer, **kwds)\n",
    "    613 if chunksize or iterator:\n",
    "    614     return parser\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448, in TextFileReader.__init__(self, f, engine, **kwds)\n",
    "   1445     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n",
    "   1447 self.handles: IOHandles | None = None\n",
    "-> 1448 self._engine = self._make_engine(f, self.engine)\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705, in TextFileReader._make_engine(self, f, engine)\n",
    "   1703     if \"b\" not in mode:\n",
    "   1704         mode += \"b\"\n",
    "-> 1705 self.handles = get_handle(\n",
    "   1706     f,\n",
    "   1707     mode,\n",
    "   1708     encoding=self.options.get(\"encoding\", None),\n",
    "   1709     compression=self.options.get(\"compression\", None),\n",
    "   1710     memory_map=self.options.get(\"memory_map\", False),\n",
    "   1711     is_text=is_text,\n",
    "   1712     errors=self.options.get(\"encoding_errors\", \"strict\"),\n",
    "   1713     storage_options=self.options.get(\"storage_options\", None),\n",
    "   1714 )\n",
    "   1715 assert self.handles is not None\n",
    "   1716 f = self.handles.handle\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/pandas/io/common.py:718, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n",
    "    715     codecs.lookup_error(errors)\n",
    "    717 # open URLs\n",
    "--> 718 ioargs = _get_filepath_or_buffer(\n",
    "    719     path_or_buf,\n",
    "    720     encoding=encoding,\n",
    "    721     compression=compression,\n",
    "    722     mode=mode,\n",
    "    723     storage_options=storage_options,\n",
    "    724 )\n",
    "    726 handle = ioargs.filepath_or_buffer\n",
    "    727 handles: list[BaseBuffer]\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/pandas/io/common.py:372, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n",
    "    370 # assuming storage_options is to be interpreted as headers\n",
    "    371 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n",
    "--> 372 with urlopen(req_info) as req:\n",
    "    373     content_encoding = req.headers.get(\"Content-Encoding\", None)\n",
    "    374     if content_encoding == \"gzip\":\n",
    "    375         # Override compression based on Content-Encoding header\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/pandas/io/common.py:274, in urlopen(*args, **kwargs)\n",
    "    268 \"\"\"\n",
    "    269 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n",
    "    270 the stdlib.\n",
    "    271 \"\"\"\n",
    "    272 import urllib.request\n",
    "--> 274 return urllib.request.urlopen(*args, **kwargs)\n",
    "\n",
    "File /opt/conda/lib/python3.11/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n",
    "    214 else:\n",
    "    215     opener = _opener\n",
    "--> 216 return opener.open(url, data, timeout)\n",
    "\n",
    "File /opt/conda/lib/python3.11/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n",
    "    523 for processor in self.process_response.get(protocol, []):\n",
    "    524     meth = getattr(processor, meth_name)\n",
    "--> 525     response = meth(req, response)\n",
    "    527 return response\n",
    "\n",
    "File /opt/conda/lib/python3.11/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n",
    "    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n",
    "    632 # request was successfully received, understood, and accepted.\n",
    "    633 if not (200 <= code < 300):\n",
    "--> 634     response = self.parent.error(\n",
    "    635         'http', request, response, code, msg, hdrs)\n",
    "    637 return response\n",
    "\n",
    "File /opt/conda/lib/python3.11/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n",
    "    561 if http_err:\n",
    "    562     args = (dict, 'default', 'http_error_default') + orig_args\n",
    "--> 563     return self._call_chain(*args)\n",
    "\n",
    "File /opt/conda/lib/python3.11/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n",
    "    494 for handler in handlers:\n",
    "    495     func = getattr(handler, meth_name)\n",
    "--> 496     result = func(*args)\n",
    "    497     if result is not None:\n",
    "    498         return result\n",
    "\n",
    "File /opt/conda/lib/python3.11/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n",
    "    642 def http_error_default(self, req, fp, code, msg, hdrs):\n",
    "--> 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
    "\n",
    "HTTPError: HTTP Error 404: Not Found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b9455",
   "metadata": {},
   "source": [
    "# The Right code of B given by Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64eb0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correcting the typo in the file name to load the Titanic dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24a540",
   "metadata": {},
   "source": [
    "# Q8(3)C wrong code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fcd7273",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Trying to use a misnamed DataFrame variable (DF instead of df)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mDF\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurvived\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DF' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correctly loading the Titanic dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Trying to use a misnamed DataFrame variable (DF instead of df)\n",
    "DF.groupby(\"class\")[\"survived\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e9847",
   "metadata": {},
   "source": [
    "# Output:\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[2], line 7\n",
    "      4 df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "      6 # Trying to use a misnamed DataFrame variable (DF instead of df)\n",
    "----> 7 DF.groupby(\"class\")[\"survived\"].describe()\n",
    "\n",
    "NameError: name 'DF' is not defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66567d39",
   "metadata": {},
   "source": [
    "# The Right code of C given by Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af6140ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>First</th>\n",
       "      <td>216.0</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.484026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Second</th>\n",
       "      <td>184.0</td>\n",
       "      <td>0.472826</td>\n",
       "      <td>0.500623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third</th>\n",
       "      <td>491.0</td>\n",
       "      <td>0.242363</td>\n",
       "      <td>0.428949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count      mean       std  min  25%  50%  75%  max\n",
       "class                                                     \n",
       "First   216.0  0.629630  0.484026  0.0  0.0  1.0  1.0  1.0\n",
       "Second  184.0  0.472826  0.500623  0.0  0.0  0.0  1.0  1.0\n",
       "Third   491.0  0.242363  0.428949  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correctly loading the Titanic dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "# Correct the DataFrame name\n",
    "df.groupby(\"class\")[\"survived\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de795a7",
   "metadata": {},
   "source": [
    "# Q8(3)D wrong code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f668aa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2040290051.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\u001b[0m\n\u001b[0m                                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Forgetting to close the parentheses in pd.read_csv\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5901fe7",
   "metadata": {},
   "source": [
    "# Output:\n",
    "Cell In[4], line 4\n",
    "    df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "                                                                                                ^\n",
    "SyntaxError: incomplete input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf5efea",
   "metadata": {},
   "source": [
    "# The Right code of D given by Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c54208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correcting the missing closing parenthesis\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac8990",
   "metadata": {},
   "source": [
    "# Q8(3)E wrong code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47aede67",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'group_by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_289/2357428460.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Loading the Titanic dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Mistyping 'groupby' as 'group_by'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"survived\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'group_by'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the Titanic dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Mistyping 'groupby' as 'group_by'\n",
    "df.group_by(\"class\")[\"survived\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be41da",
   "metadata": {},
   "source": [
    "# Output:\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "/tmp/ipykernel_289/2357428460.py in ?()\n",
    "      3 # Loading the Titanic dataset\n",
    "      4 df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "      5 \n",
    "      6 # Mistyping 'groupby' as 'group_by'\n",
    "----> 7 df.group_by(\"class\")[\"survived\"].describe()\n",
    "\n",
    "/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py in ?(self, name)\n",
    "   6200             and name not in self._accessors\n",
    "   6201             and self._info_axis._can_hold_identifiers_and_holds_name(name)\n",
    "   6202         ):\n",
    "   6203             return self[name]\n",
    "-> 6204         return object.__getattribute__(self, name)\n",
    "\n",
    "AttributeError: 'DataFrame' object has no attribute 'group_by'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e1b9bc",
   "metadata": {},
   "source": [
    "# The Right code of E given by Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "057a9b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>First</th>\n",
       "      <td>216.0</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.484026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Second</th>\n",
       "      <td>184.0</td>\n",
       "      <td>0.472826</td>\n",
       "      <td>0.500623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Third</th>\n",
       "      <td>491.0</td>\n",
       "      <td>0.242363</td>\n",
       "      <td>0.428949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count      mean       std  min  25%  50%  75%  max\n",
       "class                                                     \n",
       "First   216.0  0.629630  0.484026  0.0  0.0  1.0  1.0  1.0\n",
       "Second  184.0  0.472826  0.500623  0.0  0.0  0.0  1.0  1.0\n",
       "Third   491.0  0.242363  0.428949  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the Titanic dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Correct chaining of 'groupby' and 'describe'\n",
    "df.groupby(\"class\")[\"survived\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012ac38",
   "metadata": {},
   "source": [
    "# Q8(3)F wrong code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeef4276",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Mistyping the 'sex' column as 'Sex'\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Sex'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the Titanic dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Mistyping the 'sex' column as 'Sex'\n",
    "df.groupby(\"Sex\")[\"age\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321b4ce",
   "metadata": {},
   "source": [
    "# Output:\n",
    "    --------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[5], line 7\n",
    "      4 df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "      6 # Mistyping the 'sex' column as 'Sex'\n",
    "----> 7 df.groupby(\"Sex\")[\"age\"].describe()\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869, in DataFrame.groupby(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\n",
    "   8866 if level is None and by is None:\n",
    "   8867     raise TypeError(\"You have to supply one of 'by' and 'level'\")\n",
    "-> 8869 return DataFrameGroupBy(\n",
    "   8870     obj=self,\n",
    "   8871     keys=by,\n",
    "   8872     axis=axis,\n",
    "   8873     level=level,\n",
    "   8874     as_index=as_index,\n",
    "   8875     sort=sort,\n",
    "   8876     group_keys=group_keys,\n",
    "   8877     observed=observed,\n",
    "   8878     dropna=dropna,\n",
    "   8879 )\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278, in GroupBy.__init__(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\n",
    "   1275 self.dropna = dropna\n",
    "   1277 if grouper is None:\n",
    "-> 1278     grouper, exclusions, obj = get_grouper(\n",
    "   1279         obj,\n",
    "   1280         keys,\n",
    "   1281         axis=axis,\n",
    "   1282         level=level,\n",
    "   1283         sort=sort,\n",
    "   1284         observed=False if observed is lib.no_default else observed,\n",
    "   1285         dropna=self.dropna,\n",
    "   1286     )\n",
    "   1288 if observed is lib.no_default:\n",
    "   1289     if any(ping._passed_categorical for ping in grouper.groupings):\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009, in get_grouper(obj, key, axis, level, sort, observed, validate, dropna)\n",
    "   1007         in_axis, level, gpr = False, gpr, None\n",
    "   1008     else:\n",
    "-> 1009         raise KeyError(gpr)\n",
    "   1010 elif isinstance(gpr, Grouper) and gpr.key is not None:\n",
    "   1011     # Add key to exclusions\n",
    "   1012     exclusions.add(gpr.key)\n",
    "\n",
    "KeyError: 'Sex'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc007ad1",
   "metadata": {},
   "source": [
    "# The Right code of F given by Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cdf16ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>261.0</td>\n",
       "      <td>27.915709</td>\n",
       "      <td>14.110146</td>\n",
       "      <td>0.75</td>\n",
       "      <td>18.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>453.0</td>\n",
       "      <td>30.726645</td>\n",
       "      <td>14.678201</td>\n",
       "      <td>0.42</td>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count       mean        std   min   25%   50%   75%   max\n",
       "sex                                                              \n",
       "female  261.0  27.915709  14.110146  0.75  18.0  27.0  37.0  63.0\n",
       "male    453.0  30.726645  14.678201  0.42  21.0  29.0  39.0  80.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the Titanic dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Correcting the column name 'Sex' to 'sex'\n",
    "df.groupby(\"sex\")[\"age\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0967d",
   "metadata": {},
   "source": [
    "# Q8(3)G wrong code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e53aec85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Forgetting to put 'sex' in quotes\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[43msex\u001b[49m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sex' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the Titanic dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Forgetting to put 'sex' in quotes\n",
    "df.groupby(sex)[\"age\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b7044",
   "metadata": {},
   "source": [
    "# Output:\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[7], line 7\n",
    "      4 df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "      6 # Forgetting to put 'sex' in quotes\n",
    "----> 7 df.groupby(sex)[\"age\"].describe()\n",
    "\n",
    "NameError: name 'sex' is not defined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b0747d",
   "metadata": {},
   "source": [
    "# The Right code of G given by Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb52af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>261.0</td>\n",
       "      <td>27.915709</td>\n",
       "      <td>14.110146</td>\n",
       "      <td>0.75</td>\n",
       "      <td>18.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>453.0</td>\n",
       "      <td>30.726645</td>\n",
       "      <td>14.678201</td>\n",
       "      <td>0.42</td>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count       mean        std   min   25%   50%   75%   max\n",
       "sex                                                              \n",
       "female  261.0  27.915709  14.110146  0.75  18.0  27.0  37.0  63.0\n",
       "male    453.0  30.726645  14.678201  0.42  21.0  29.0  39.0  80.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the Titanic dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')\n",
    "\n",
    "# Correctly putting 'sex' in quotes\n",
    "df.groupby(\"sex\")[\"age\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1940343d",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "When I ask ChatGPT about the error \"NameError: name 'pd' is not defined,\" it provides a clear explanation of the issue, how to fix it, and a correct example. In contrast, when I search on Google, it returns many results from different websites. While the Google answers also include explanations, fixes, and tutorial videos, they often come with irrelevant information like ads. In this case, using ChatGPT is faster, as it offers an immediate solution without having to sift through multiple Google results. Additionally, ChatGPT helps filter the information from the internet, eliminating the less useful content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1f46d",
   "metadata": {},
   "source": [
    "# Summaries of ChatBot session of prelecture all question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a24da4",
   "metadata": {},
   "source": [
    "Hereâ€™s a summary of our exchanges that you can submit as part of your homework assignment:\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Interactions**\n",
    "\n",
    "1. **Discussing `df.describe()` summary statistics**:\n",
    "   We began by discussing the definitions of the statistical metrics provided by `df.describe()` in pandas, including count, mean, std, min, 25%, 50%, 75%, and max.\n",
    "\n",
    "2. **Use case for `df.dropna()` vs. `del df['col']`**:\n",
    "   You requested an explanation of a use case where `df.dropna()` is preferred over `del df['col']`. We explored scenarios where it's better to remove rows with missing values while retaining the column structure.\n",
    "   \n",
    "   In contrast, we then discussed a use case where `del df['col']` is preferred when a column has too many missing values and is not critical for analysis.\n",
    "\n",
    "3. **Error handling in pandas**:\n",
    "   You introduced several errors into your pandas code and asked me to troubleshoot:\n",
    "   \n",
    "   - **NameError**: Mistyping `pd` as unimported.\n",
    "   - **FileNotFoundError**: Incorrect file paths and file names, which led to troubleshooting file locations and paths.\n",
    "   - **SyntaxError**: Forgetting to close parentheses in `pd.read_csv()`.\n",
    "   - **AttributeError**: Mistyping `groupby` as `group_by`.\n",
    "   - **KeyError**: Using a non-existent column like `'Sex'` instead of `'sex'`.\n",
    "   - **NameError**: Forgetting to put a column name (like `'sex'`) in quotes.\n",
    "\n",
    "4. **Fixing the errors**:\n",
    "   For each error, I explained why it occurred and how to fix it. For example:\n",
    "   \n",
    "   - We fixed the **`FileNotFoundError`** by using the correct URL for the Titanic dataset (`https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv`).\n",
    "   - We corrected **`SyntaxError`** by adding missing parentheses.\n",
    "   - We resolved **`KeyError`** and **`AttributeError`** by correcting typos in column names and method names.\n",
    "\n",
    "5. **Comparison of ChatBot vs. Google for error troubleshooting**:\n",
    "   You asked whether it's more efficient to troubleshoot errors using a chatbot or Google search. We concluded that for common coding errors (like typos, missing imports, and path issues), chatbots provide quick, context-aware fixes. However, for more complex or unusual issues, a Google search might be more effective due to the availability of community input.\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to use this summary as a record of our interactions. Let me know if you'd like any further edits or additional information for submission!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23146f",
   "metadata": {},
   "source": [
    "# link to chat log histories:\n",
    "https://chatgpt.com/c/66e3094e-eb54-8001-8df0-16140356f375"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c25e88",
   "metadata": {},
   "source": [
    "# Q9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ff723",
   "metadata": {},
   "source": [
    "Yes, I have reviewed the course wiki-textbook and interacted with a ChatBot (such as ChatGPT) to clarify concepts I initially struggled with in the tutorials and lectures. Additionally, I have participated in discussions on Piazza to gain further insights into the material. Until now, I didn't attend TA's office hours, I plan attend the office hours next week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
